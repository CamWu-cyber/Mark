# 349. 两个数组的交集
给定两个数组 nums1 和 nums2 ，返回 它们的交集 。输出结果中的每个元素一定是 唯一 的。我们可以 不考虑输出结果的顺序 。

示例 1：

    输入：nums1 = [1,2,2,1], nums2 = [2,2]
    输出：[2]
示例 2：

    输入：nums1 = [4,9,5], nums2 = [9,4,9,8,4]
    输出：[9,4]
    解释：[4,9] 也是可通过的

思路：求交集用 & 与运算，只对set和dic有效果。

    class Solution:
        def intersection(self, nums1, nums2):
            return list(set(nums1) & set(nums2))

    if __name__ == '__main__':
        obj = Solution()
        print(obj.intersection([1,2,2,1], [2,2]))
        
#### 运行结果
    [2]
    
### C++
这道题目，主要要学会使用一种哈希数据结构：unordered_set，这个数据结构可以解决很多类似的问题。

**使用数组来做哈希的题目，是因为题目都限制了数值的大小。** 而这道题目没有限制数值的大小，就无法使用数组来做哈希表了。

**而且如果哈希值比较少、特别分散、跨度非常大，使用数组就造成空间的极大浪费。**

此时就要使用另一种结构体了，set ，关于set，C++ 给提供了如下三种可用的数据结构：

* std::set
* std::multiset
* std::unordered_set

std::set和std::multiset底层实现都是红黑树，std::unordered_set的底层实现是哈希表， 使用unordered_set 读写效率是最高的，并不需要对数据进行排序，而且还不要让数据重复，所以选择unordered_set。

1. nums1使用unordered_set去重
2. 遍历nums2，如果在去重的结果里面，就把该元素放入结果中。
3. 返回结果。

        #include<iostream>
        #include<unordered_set>
        using namespace std;

        class Solution {
        public:
            vector<int> intersection(vector<int>& nums1, vector<int>& nums2) {
                unordered_set<int> result;
                unordered_set<int> num_set(nums1.begin(), nums1.end());
                for (int num : nums2) {
                    // 发现nums2的元素 在nums_set里又出现过
                    if (num_set.find(num) != num_set.end()) {
                        result.insert(num);
                    }
                }
                return vector<int>(result.begin(), result.end());  // unordered_set转vector
            }
        };

        int main() {
            Solution obj;
            vector<int> nums1 = { 1,2,2,1 };
            vector<int> nums2 = { 2,2 };
            vector<int> res = obj.intersection(nums1, nums2);
            for (int r : res) {
                cout << r << " ";
            }
        }
#### 运行结果
    2

### 拓展
那有同学可能问了，遇到哈希问题我直接都用set不就得了，用什么数组啊。

直接使用set 不仅占用空间比数组大，而且速度要比数组慢，set把数值映射到key上都要做hash计算的。

不要小瞧 这个耗时，在数据量大的情况，差距是很明显的。
